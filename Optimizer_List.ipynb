{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba21f8f-1268-497a-8836-cdc0db11017e",
   "metadata": {},
   "source": [
    "# Optimizer List #\n",
    "Fairly comprehensive list of optimizers available in Tensorflow, along with relationships to ADAM article.\n",
    "\n",
    "Richard Henry, 13-Jul-2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec745da-4136-40d1-b7bc-421b2c17ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libararies\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab580137-e46a-4eca-8f26-212c51ae0970",
   "metadata": {},
   "source": [
    "Test Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3993fae-3caa-4e5d-a26b-2feaa014ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.3\n"
     ]
    }
   ],
   "source": [
    "# Check python version\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "#3.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536a1fdc-4aa4-4cc5-808e-17d633e449fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensorflow version\n",
    "tf.__version__\n",
    "#2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25744cbf-8fd4-46f3-ac63-d92177a0ebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check keras version\n",
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00e81b-ec40-4983-940f-d6e1074e8c04",
   "metadata": {},
   "source": [
    "## AdaDelta ##\n",
    "Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks:\n",
    "\n",
    "The continual decay of learning rates throughout training.\n",
    "The need for a manually selected global learning rate.\n",
    "Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done.\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Yes|\n",
    "|Year|2012|\n",
    "|Short Cut|'adadelta'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006341cb-fd10-4664-a1b9-5f5233059d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt001=tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.95,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adadelta'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e155a-dca7-495b-80c7-8249d1d6d98b",
   "metadata": {},
   "source": [
    "# AdaFactor #\n",
    "Adafactor is commonly used in NLP tasks, and has the advantage of taking less memory because it only saves partial information of previous gradients.\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|No|\n",
    "|Year|2018|\n",
    "|Short Cut|'adafactor'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aeee568-3ae9-4624-99ea-9d0862e4fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt002=tf.keras.optimizers.Adafactor(\n",
    "    learning_rate=0.001,\n",
    "    beta_2_decay=-0.8,\n",
    "    epsilon_1=1e-30,\n",
    "    epsilon_2=0.001,\n",
    "    clip_threshold=1.0,\n",
    "    relative_step=True,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adafactor'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f310b7f-9c2d-4d41-bbd6-ec2da4d52182",
   "metadata": {},
   "source": [
    "# AdaGrad #\n",
    "Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Yes|\n",
    "|Year|2011|\n",
    "|Short Cut|'adagrad'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af21c3a-af33-403a-8a90-e4a6d0523218",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt003=tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adagrad'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b523f3-22fa-4c9f-9874-2dfb02070f29",
   "metadata": {},
   "source": [
    "# ADAM #\n",
    "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|duh|\n",
    "|Year|2015|\n",
    "|Short Cut|'adam'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c927b91-a776-4dcb-9645-85995d23910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt000=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe9989-324c-4e5a-bb72-9b31524464e3",
   "metadata": {},
   "source": [
    "# ADAM with Weight Decay #\n",
    "AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights.\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|No|\n",
    "|Year|2019|\n",
    "|Short Cut|'adamw'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0b06e1-aff6-4591-ad6c-57b4360e3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt004=tf.keras.optimizers.AdamW(\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.004,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adamw'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768512f8-67b3-4d8d-a85b-773065a49728",
   "metadata": {},
   "source": [
    "# AdaMax #\n",
    "A variant of Adam described in the same paper and based on the infinity norm, is a first-order gradient-based optimization method. Due to its capability of adjusting the learning rate based on data characteristics, it is suited to learn time-variant process, e.g., speech data with dynamically changed noise conditions. \n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Yes|\n",
    "|Year|2015|\n",
    "|Short Cut|'adamax'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5d0635-718e-4a94-a8d2-b8fcce54e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt005=tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adamax'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03bc9e-1f28-46e7-b11a-6b878ad95399",
   "metadata": {},
   "source": [
    "# Follow the Regularized Leader #\n",
    "An optimization algorithm developed at Google for click-through rate prediction. It is most suitable for shallow models with large and sparse feature spaces.\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|No|\n",
    "|Year|2013|\n",
    "|Short Cut|'ftrl'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb97fa2-ae0b-4cab-ac57-772ad9bc7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt006=tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.001,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.0,\n",
    "    l2_regularization_strength=0.0,\n",
    "    l2_shrinkage_regularization_strength=0.0,\n",
    "    beta=0.0,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='ftrl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288d672-d531-43de-a17e-bd68e24c1edd",
   "metadata": {},
   "source": [
    "# Lion #\n",
    "A stochastic-gradient-descent method that uses the sign operator to control the magnitude of the update, unlike other adaptive optimizers such as Adam that rely on second-order moments. This make Lion more memory-efficient as it only keeps track of the momentum.\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Hell No|\n",
    "|Year|2023|\n",
    "|Short Cut|'lion'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612b8515-e59c-4ec9-94e1-fd1aa37af895",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt007=tf.keras.optimizers.Lion(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.99,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='lion'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1c98-2ac5-4859-81cd-f16df88c252c",
   "metadata": {},
   "source": [
    "# Adam with Nesterov momentum #\n",
    "\n",
    "Momentum is an approach that accelerates the progress of the search to skim across flat areas and smooth out bouncy gradients. [Brownlee]\n",
    "Nesterov momentum is an extension of momentum that involves calculating the decaying moving average of the gradients of projected positions in the search space rather than the actual positions themselves. [Also Brownlee]\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|No|\n",
    "|Year|2015|\n",
    "|Short Cut|'nadam'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e80d4e4-578d-4f54-b127-83850b3f9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt008=tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='nadam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e3263-c851-4b3c-89df-db5faf11b3ce",
   "metadata": {},
   "source": [
    "# Root Mean Square Propagation #\n",
    "ADAM is based on this.The gist of RMSprop is to:\n",
    "\n",
    "1. Maintain a moving (discounted) average of the square of gradients\n",
    "2. Divide the gradient by the root of this average\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Yup|\n",
    "|Year|2012|\n",
    "|Short Cut|'rmsprop'|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58898afe-4800-4a7b-b19e-c29fc72ef34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt009=tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='rmsprop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4e701-3df8-45d4-86e1-7d9d0c5cb0c7",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent #\n",
    "Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the minimum of the function. [Brownlee] Instead of updating weights based on the sum of the accumulated errors over all training examples, we update parameters incrementally for each training example. Although SGD can be considered an approximation of gradient descent, it typically reaches convergence faster because of more frequent weight updates.[Raschka]\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Yes|\n",
    "|Year|a really long time ago|\n",
    "|Short Cut|'SGD'<-- notice CAPS!|\n",
    "|Default Setting|Yes|\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30e0893-ecbe-4812-938d-28de4a2f6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt010=tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='SGD'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8684418-f818-4296-9798-7ca94d754b15",
   "metadata": {},
   "source": [
    "# SGD with Momentum #\n",
    "Mentioned in Article.  Increase `momentum` above zero and below unity. Not sure it is used.\n",
    "# SGD with Nesterov Momentum #\n",
    "Mentioned in Article.  Increase `momentum` above zero and below unity. Set `nesterov` to `True`.\n",
    "# RMSprop with Momentum #\n",
    "Mentioned in Article.  Increase `momentum` above zero and below unity.  Notice we can't use Nesterov here.\n",
    "# Natural Gradient Descent #\n",
    "\n",
    "|Variable|Value|\n",
    "|---|---|\n",
    "|Mentioned in Article|Unfortunately, yes|\n",
    "|Year|1998|\n",
    "|Short Cut|'NGD'|\n",
    "|Default Setting|No clue|\n",
    "\n",
    "Can we like, ignore this one?  Please?\n",
    "\n",
    "https://www.geeksforgeeks.org/optimizers-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03fd6a9-d532-406f-b10f-5cb2509e2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brera\\AppData\\Local\\Temp\\ipykernel_24580\\2360995615.py:1: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opt999=tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001, \n",
    "                                                    use_locking=True,\n",
    "                                                    name = 'NGD')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
